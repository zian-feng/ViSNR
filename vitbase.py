# -*- coding: utf-8 -*-
"""ViTbase.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17rvEpQRBu3EAAnBK-vl18FxGo1fJSN4l
"""

from google.colab import drive
drive.mount('/content/drive')

!python --version

import os
os.getcwd()

# os.chdir('/content/drive/MyDrive/XRA')
# os.getcwd()

!pip install -r requirements.txt

!pip install monai
!pip install torchmetrics

!pip install opencv-python

"""import packages & dependencies"""

import os

import matplotlib.pyplot as plt
import numpy as np
from torch.utils.data import DataLoader, Dataset
from torchvision.io import read_image
from torchvision import transforms
from torch.utils.data import DataLoader
from torchvision.transforms import InterpolationMode

import torch
import torch.nn as nn
from transformers import ViTModel, AutoConfig, ViTImageProcessor, ViTConfig

from transformers import Trainer, TrainingArguments, AutoModelForSemanticSegmentation, AutoTokenizer
from transformers import EarlyStoppingCallback


from safetensors.torch import load_file

import monai.losses as ml               # monai loss functions library
import monai.metrics as mm              # monai metrics library
from monai.networks.utils import one_hot

from torchmetrics.segmentation import DiceScore, HausdorffDistance

from monai.losses import FocalLoss, TverskyLoss

from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, jaccard_score

from skimage.filters import sato
import cv2

from transformers.models.vit.modeling_vit import ViTAttention, ViTSelfAttention

"""# Image Data & Processing"""

# loading data
# ABS_PATH = '/Users/daofeng/Desktop/______/INM363/CODE/syntax'               # ABS path of syntax df
ABS_PATH = '/content/drive/MyDrive/XRA/syntax'

# define custom data class
class Syntax():
    def __init__(self, root_path, dataset):

        self.root_path =  root_path

        if dataset == 'train':

            self.images = sorted([root_path + '/train/images/' + i for i in os.listdir(root_path + '/train/images/') if not i.startswith('.')],
                                 key = lambda x: int(os.path.splitext(os.path.basename(x))[0]))

            self.masks  = sorted([root_path + '/train/masks/' + i for i in os.listdir(root_path + '/train/masks/') if not i.startswith('.')],
                                 key = lambda x: int(os.path.splitext(os.path.basename(x))[0]))

            self.labels = sorted(os.listdir(root_path + '/train/images/'), key = lambda x: int(os.path.splitext(x)[0]))
            # self.annots = root_path + '/train/annotations/' + 'train.json'

        elif dataset == 'val':

            self.images = sorted([root_path + '/val/images/' + i for i in os.listdir(root_path + '/val/images/') if not i.startswith('.')],
                                 key = lambda x: int(os.path.splitext(os.path.basename(x))[0]))

            self.masks  = sorted([root_path + '/val/masks/' + i for i in os.listdir(root_path + '/val/masks/') if not i.startswith('.')],
                                 key = lambda x: int(os.path.splitext(os.path.basename(x))[0]))

            self.labels = sorted(os.listdir(root_path + '/val/images/'), key = lambda x: int(os.path.splitext(x)[0]))

        elif dataset == 'test':

            self.images = sorted([root_path + '/test/images/' + i for i in os.listdir(root_path + '/test/images/') if not i.startswith('.')],
                                 key = lambda x: int(os.path.splitext(os.path.basename(x))[0]))

            self.masks  = sorted([root_path + '/test/masks/' + i for i in os.listdir(root_path + '/test/masks/') if not i.startswith('.')],
                                 key = lambda x: int(os.path.splitext(os.path.basename(x))[0]))

            self.labels = sorted(os.listdir(root_path + '/test/images/'), key = lambda x: int(os.path.splitext(x)[0]))

        else:
            raise ValueError("dataset parameter needs to be 'train', 'val', or 'test' ")
            # pass


        self.transform = transforms.Compose([
            transforms.Resize((224,224)),                                  # original size 512x512,
            transforms.Grayscale(num_output_channels = 1),                 # converts all to grayscale (1 x 224 x 224), input for vit needs 3 channels
            transforms.ConvertImageDtype(torch.float32)                    # convert to torch tensor
        ])

        self.ch3_transform = transforms.Compose([                          # second transform layer for images
            transforms.Lambda(lambda x: x.repeat(3, 1, 1))                 # this converts 1ch to 3ch stacked (3C, H, W)
        ])

        self.msk_transform = transforms.Compose([
            # transforms.Resize((224,224),
            #                   interpolation = InterpolationMode.NEAREST),                                  # original size 512x512
            transforms.ConvertImageDtype(torch.float32),                    # convert to torch tensor
            transforms.Lambda(lambda pixel: (pixel > 0).float())            # binarize masks
        ])


    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):

        img = read_image(self.images[idx])                  # loads images
        img = self.transform(img)                           # applies transforms
        img = self.ch3_transform(img)                       # converts to 3 channels



        msk = read_image(self.masks[idx])                   # loads masks
        msk = self.transform(msk)                           # applies transforms

        lbl = self.labels[idx]

        return img, msk, lbl

# initialize datasets
data_train = Syntax(root_path = ABS_PATH, dataset = 'train')
data_val = Syntax(root_path = ABS_PATH, dataset = 'val')
data_test = Syntax(root_path = ABS_PATH, dataset = 'test')

# define data loaders
BATCHN = [10, 20, 25, 50, 100]                       # different batch sizes batch sizes

train_loader = DataLoader(dataset = data_train, shuffle = False, batch_size = BATCHN[2])
test_loader = DataLoader(dataset = data_test, shuffle = False, batch_size = BATCHN[3])
val_loader = DataLoader(dataset = data_val, shuffle = False, batch_size = BATCHN[4])

train_imgs, train_msks, train_lbls = next(iter(train_loader))
val_imgs, val_msks, val_lbls = next(iter(val_loader))

# load full train/val/test data from dataloaders
data_train
data_val
data_test

def load_all(loader):
    imgs, msks, lbls = [], [], []

    for i, m, l in loader:
        imgs.append(i)
        msks.append(m)
        lbls.append(l)

    imgs = torch.cat(imgs, dim = 0)             # concatenate batches
    msks = torch.cat(msks, dim = 0)

    return imgs, msks, lbls


train_imgs, train_msks, train_lbls = load_all(train_loader)
val_imgs, val_msks, val_lbls = load_all(val_loader)
test_imgs, test_msks, test_lbls = load_all(test_loader)

# define sato + sobel filter function

def apply_filters(img_ds):

    output = []

    for img in img_ds:
        grayscale_ch = img[0].numpy()               # grayscale channel to np array

        # # clahe channel
        # clahe_ch = cv2.createCLAHE(clipLimit = 2, tileGridSize = (8, 8))

        # sato filter
        sato_ch = sato(grayscale_ch, sigmas=(1,2,3), black_ridges=True)     # black_ridges -> dark edges, sigmas -> gaussian std. dev.
        sato_ch = np.clip(sato_ch, 0.0, 1.0).astype(np.float32)             # min-max clipped at [0, 1]

        # sobel filter
        grad_x = cv2.Sobel(grayscale_ch, cv2.CV_32F, 1, 0, ksize = 3)       # kernel 3x3 convolutions
        grad_y = cv2.Sobel(grayscale_ch, cv2.CV_32F, 0, 1, ksize = 3)       # CV_32F -> np.float32

        grad_m = cv2.magnitude(grad_x, grad_y)                              # gradient magnitude
        grad_m = grad_m / (grad_m.max() + 1e-6)                             # normalize w/ epsilon 1e-6
        sobel_ch = grad_m.astype(np.float32)

        stacked_ch = np.stack([grayscale_ch, sato_ch, sobel_ch], axis = 0)  # axis 0 stack on channel dim

        output.append(torch.from_numpy(stacked_ch))                         # append to output

    return torch.stack(output)

# apply fiters
train_imgs = apply_filters(train_imgs)
val_imgs = apply_filters(val_imgs)
test_imgs = apply_filters(test_imgs)

# compute normalizations
# use train img mean & std              train_imgs[:, 0, :, :].std().item()

grayscale_mean = [train_imgs[:, 0, :, :].mean().item(),
                  train_imgs[:, 0, :, :].mean().item(),
                  train_imgs[:, 0, :, :].mean().item()]

grayscale_std = [train_imgs[:, 0, :, :].std().item(),
                 train_imgs[:, 0, :, :].std().item(),
                 train_imgs[:, 0, :, :].std().item()]

# channel wise mean
channel_mean = [train_imgs[:, 0, :, :].mean().item(),
                train_imgs[:, 1, :, :].mean().item(),
                train_imgs[:, 2, :, :].mean().item()]

channel_std = [train_imgs[:, 0, :, :].std().item(),
               train_imgs[:, 1, :, :].std().item(),
               train_imgs[:, 2, :, :].std().item()]

# use processor and collator to prepare the images
processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')
# model_input = processor(images = train_imgs, return_tensors = 'pt')

# processor settings
# processor.size = {'height': 512, 'width': 512}
processor.size = {'height': 224, 'width': 224}
processor.do_convert_rgb = False        # grayscale / custom channels
processor.do_rescale = True


# processor.image_mean = grayscale_mean           # defaults to [0.5, 0.5, 0.5]
# processor.image_std = grayscale_std
# processor.do_normalize = True

processor.image_mean = channel_mean           # defaults to [0.5, 0.5, 0.5]
processor.image_std = channel_std
processor.do_normalize = True

# processing each dataset for trainer
train_x = processor(images = train_imgs, return_tensors = 'pt')
train_x = train_x['pixel_values']
train_y = train_msks

val_x = processor(images = val_imgs, return_tensors = 'pt')
val_x = val_x['pixel_values']
val_y = val_msks

test_x = processor(images = test_imgs, return_tensors = 'pt')
test_x = test_x['pixel_values']
test_y = test_msks

# collate into one dataset dict of X, y
class SYN(Dataset):
    def __init__(self, pixel_values: torch.Tensor, masks: torch.Tensor):
        self.x = pixel_values
        self.y = masks

    def __len__(self):
        return self.y.size(0)

    def __getitem__(self, idx):
        dat = {
            'pixel_values': self.x[idx],
            'labels': self.y[idx]
        }
        return dat

ds_train = SYN(train_x, train_y)
ds_val = SYN(val_x, val_y)
ds_test = SYN(test_x, test_y)

"""# Defining Model"""

model_id = 'google/vit-base-patch16-224'
# model_id = 'google/vit-base-patch32-224-in21k'
config = ViTConfig.from_pretrained(model_id)
VTmodel = ViTModel.from_pretrained(model_id)

# config = ViTConfig(
#     hidden_size = 768,
#     num_hidden_layers = 12,
#     num_attention_heads = 12,
#     image_size = 512,
#     patch_size = 32,
#     hidden_dropout_prob = 0.0,
#     attention_probs_dropout_prob = 0.0,
#     qkv_bias = True
# )

config = ViTConfig(
    hidden_size = 768,
    num_hidden_layers = 12,
    num_attention_heads = 12,
    image_size = 224,
    patch_size = 16,
    hidden_dropout_prob = 0.0,
    attention_probs_dropout_prob = 0.0,
    qkv_bias = True
)

class ViT(nn.Module):
    def __init__(self, model_id, img_size, patch_size, freeze):
        super().__init__()

        # loading the pre-trained model
        # self.config = ViTConfig.from_pretrained(model_id)

        self.config = ViTConfig(
            hidden_size = 768,
            num_hidden_layers = 12,
            num_attention_heads = 12,
            image_size = 224,
            patch_size = 16,
            hidden_dropout_prob = 0.0,
            attention_probs_dropout_prob = 0.0,
            qkv_bias = True
            )


        # self.vit = ViTModel(self.config)
        self.vit = ViTModel.from_pretrained(model_id, config = self.config)

        # self.embeddings = self.vit.embeddings
        # self.vit.get_position_embeddings



        # define params
        self.img_size = 224                                 # default ViT input size, (original size: 512x512)
        self.patch_size = 16                                # default 16 patches
        self.num_patches = img_size // patch_size           # number of patches in image (14 patches)
        self.grid_size = (img_size // patch_size) ** 2      # grid size of each patch (14x14 = 196)

        # backbone vit encoder param freeze
        if freeze:
            for par in self.vit.parameters():
                par.requires_grad = False                   # ViT params/weights frozen, only trains decoder
            print('encoder backbone frozen')
        else:
            print('encoder backbone training enabled')


        # define decoder
        self.decoder = nn.Sequential(
            nn.LayerNorm(768),                                # input dim [B, grid_size = 196, hidden = 768]
            nn.Linear(768, 1024),                             # fc layer to [B, 196, 1024]
            nn.GELU(),                                        # gelu activation
            nn.Dropout(0.1),
            nn.Linear(1024, 512),                             # [B, 196, 512]
            nn.GELU(),                                        # gelu activation
            nn.Dropout(0.1),
            nn.Linear(512, 64),                               # [B, 196, 64]
        )


        # define spatial upsampling
        self.upsample = nn.Sequential(
            # nn.convTranspose2d(in_channels, out_channels, kernel_size, stride, padding)
            # dimensions [B, C, H, W]
            nn.ConvTranspose2d(64, 128, kernel_size = 4, stride = 2, padding = 1),         # [B, 64->128, 14->28, 14->28]
            nn.BatchNorm2d(128),                                                           # batch norm applied to 128 channels
            nn.ReLU(),
            nn.ConvTranspose2d(128, 64, kernel_size = 4, stride = 2, padding = 1),         # [B, 128->64, 28->56, 28->56]
            nn.BatchNorm2d(64),                                                           # batch norm applied to 64 channels
            nn.ReLU(),
            nn.ConvTranspose2d(64, 32, kernel_size = 4, stride = 2, padding = 1),         # [B, 64->32, 56->112, 56->112]
            nn.BatchNorm2d(32),                                                           # batch norm applied to 32 channels
            nn.ReLU(),
            nn.ConvTranspose2d(32, 16, kernel_size = 4, stride = 2, padding = 1),         # [B, 32->16, 112->224, 112->224]
            nn.BatchNorm2d(16),                                                           # batch norm applied to 16 channels
            nn.ReLU(),

            nn.Conv2d(16, 1, kernel_size = 1),                                            # [B, 1, 224, 224]
            )


        # define loss functions

        # bce + dice


        # self.loss_bce = nn.BCEWithLogitsLoss(pos_weight = torch.tensor((1 - 0.0196) / 0.0196), reduction='mean')  # weighted where foreground % is 1.96
        # # self.loss_dice = ml.dice.DiceLoss(sigmoid = True, squared_pred = True, reduction = 'mean', smooth_dr = 1e-5, smooth_nr = 1e-5)
        # self.loss_dice = ml.dice.DiceLoss(sigmoid = True, reduction = 'mean', smooth_dr = 1e-5, smooth_nr = 1e-5)

        # self.a = 0.4

        # self.combined_loss = lambda y_pred, y_true: self.a * self.loss_bce(y_pred, y_true) + (1 - self.a)* self.loss_dice(y_pred, y_true).mean()

        # # combined weighted bce + dice loss
        # self.loss_function = self.combined_loss

        self.loss_focal = FocalLoss(gamma = 2, alpha = 0.90, weight = None,
                                    reduction = 'mean')

        self.loss_tversky = TverskyLoss(alpha = 0.6, beta = 0.4, reduction = 'mean',
                                        sigmoid = True, smooth_nr = 1e-4, smooth_dr = 1e-4,)

        self.a = 0.6

        self.combined_loss = lambda y_pred, y_true: self.a * self.loss_focal(y_pred, y_true) + (1 - self.a)* self.loss_tversky(y_pred, y_true)

        # combined weighted focal + tversky loss
        self.loss_function = self.combined_loss



    def forward(self, pixel_values, labels = None):


        encoder_outputs = self.vit(pixel_values, return_dict = True)        # hidden size 768, vit out dim [B, 196 + 1 cls token, 768]
        patch_embeddings = encoder_outputs.last_hidden_state[:, 1:, :]      # [batch, grid_size, hidden] indexed from 1 b/c CLS token prepended to grid_size at pos 0
        patch_features = self.decoder(patch_embeddings)                     # passes embeddings into decoder w/ shape [B, 196, 768] -> [B, 196, 64]

        batch_size = patch_features.shape[0]                                # returns batch size

        spatial_logits = patch_features.transpose(1, 2).reshape(
            batch_size, 64, self.num_patches, self.num_patches)              # transpose features [B, 196, 64] -> [B, 64, 196] and reshape to [B, C = 64, 14, 14]


        ups_logits = self.upsample(spatial_logits)                          # upsample to original size [B, 1, 224, 244]


        if labels is not None:                                              # if ground truth mask is provided (train/val)
            labels = (labels > 0.5).float()
            loss = self.loss_function(ups_logits, labels)
            return {'loss': loss, "logits": ups_logits}                     # return loss and logits for training

        else:
            return ups_logits                                               # return only logits


    def predict(self, pixel_values, threshold):
        with torch.no_grad():
            logits = self.forward(pixel_values)                             # computes logits in forward pass
            probas = torch.sigmoid(logits)                                  # computes probabilities from logits
            bin_msk = (probas > threshold).float()                          # creates binary mask w/ threshold value
        return bin_msk                                                      # bin_msk y pred

    def unfreeze(self):
        for par in self.vit.parameters():
            par.requires_grad = True                                         # unfreeze all ViT encoder backbone for full fine tuning
        print('vit encoder unfrozen')

"""# Training"""

dice_mm = mm.DiceMetric(include_background=False, reduction = 'mean', get_not_nans=False)

# define eval metrics

def eval_metrics(evalpred):
    logits = evalpred.predictions                           # returns model pred on val data
    y_true = evalpred.label_ids                             # returns gt mask on val data

    probas = 1 / (1 + np.exp(-logits))                      # sigmoid function
    y_pred = (probas > 0.2).astype(np.float32)              # convert to binary

    y_pred_fl = y_pred.ravel().astype(int)                  # flatten for sklearn
    y_true_fl = y_true.ravel().astype(int)

    y_pred_tensor = torch.from_numpy(y_pred.astype(np.float32))
    y_true_tensor = torch.from_numpy(y_true.astype(np.float32))

    y_pred_1hot = one_hot(y_pred_tensor, num_classes = 2)
    y_true_1hot = one_hot(y_true_tensor, num_classes = 2)

    # compute metrics
    acc = accuracy_score(y_pred_fl, y_true_fl)
    f1 = f1_score(y_pred_fl, y_true_fl, zero_division = 0)
    prec = precision_score(y_pred_fl, y_true_fl, zero_division = 0)
    rec = recall_score(y_pred_fl, y_true_fl, zero_division = 0)
    js = jaccard_score(y_pred_fl, y_true_fl, zero_division = 0)

    # dice and iou scores
    # dice_score = DiceScore(num_classes = 2, include_background = False)
    # dice = dice_score(y_pred_tensor, y_true_tensor)

    dice_score = dice_mm(y_pred_1hot, y_true_1hot)
    dice = dice_mm.aggregate().item()
    dice_mm.reset()                                       # reset dice for next epoch


    # hausdorff = HausdorffDistance(num_classes = 2, include_background = False,
    #                        distance_metric = 'euclidean', directed = False)

    # hd = hausdorff(y_pred_tensor, y_true_tensor)


    res_dict = {'acc': acc, 'dice': dice, 'f1': f1, 'rec': rec, 'prec': prec, 'jacc': js }

    return res_dict

vit = ViT(model_id, img_size = 224, patch_size = 16, freeze = True)

# send to gpu
torch.backends.mps.is_built()                       # check that mps build is compliant w/ pytorch

if torch.backends.mps.is_available():
    device = torch.device('mps')
else:
    device = torch.device('cpu')

print(device)

vit.to(device)

targs1 = TrainingArguments(
    output_dir = 'training/vitbase/p1',                     # separate training output
    # logging_dir = 'training/vit-b16/logs',
    report_to = ['none'],
    num_train_epochs = 15,                               # training decoder, use more epochs
    per_device_train_batch_size = 25,
    per_device_eval_batch_size = 25,

    learning_rate = 1e-4 ,                               # try 5e-4 for bigger steps
    weight_decay = 0.01 ,

    warmup_ratio = 0.10,
    lr_scheduler_type='cosine',

    # using fp32 -- full precision
    fp16 = False,                                       # disable half precision
    bf16 = False,                                       # disable bfloat precision

    logging_strategy = 'epoch',
    save_strategy = 'epoch',
    eval_strategy = 'epoch',
    load_best_model_at_end = True,                      # trainer saves model
    metric_for_best_model = 'eval_loss',                # ['eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch']
    greater_is_better = False,                           # false for BCE + dice loss, Focal + Tversky

    save_total_limit = 3,
    save_safetensors = True,
)

# instantiate trainer
trainer1 = Trainer(
    model = vit,
    args = targs1,
    train_dataset = ds_train,
    eval_dataset = ds_val,
    compute_metrics = eval_metrics,
    callbacks = [EarlyStoppingCallback(early_stopping_patience = 5)]
)

from accelerate.state import AcceleratorState
from accelerate import Accelerator


AcceleratorState._reset_state()
accel = Accelerator()

trainer1.train()

# reinstantiate
vitp2 = ViT(model_id, img_size = 224, patch_size = 16, freeze = False)

# load params
checkpoint = trainer1.state.best_model_checkpoint
# checkpoint = 'training/vit16/phase_1/checkpoint-2760'

# load safetensors file
w_path = checkpoint + '/model.safetensors'

print(w_path)
# load weights into model
state = load_file(w_path, device = 'cpu')
vitp2.load_state_dict(state)

print(device)
vitp2.to(device)

targs2 = TrainingArguments(
    output_dir = 'training/vitp2/p2',                     # separate training output
    # logging_dir = 'training/vit-b16/logs',
    report_to = ['none'],
    num_train_epochs = 100,                               # training decoder, use more epochs
    per_device_train_batch_size = 25,
    per_device_eval_batch_size = 25,

    learning_rate = 1e-4 ,                               # try 5e-4 for bigger steps
    weight_decay = 0.01 ,

    warmup_ratio = 0.10,
    lr_scheduler_type='cosine',

    # using fp32 -- full precision
    fp16 = False,                                       # disable half precision
    bf16 = False,                                       # disable bfloat precision

    logging_strategy = 'epoch',
    save_strategy = 'epoch',
    eval_strategy = 'epoch',
    load_best_model_at_end = True,                      # trainer saves model
    metric_for_best_model = 'eval_loss',                # ['eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch']
    greater_is_better = False,                           # false for BCE + dice loss, Focal + Tversky

    save_total_limit = 3,
    save_safetensors = True,
)

trainer2 = Trainer(
    model = vitp2,
    args = targs2,
    train_dataset = ds_train,
    eval_dataset = ds_val,
    compute_metrics = eval_metrics,
    callbacks = [EarlyStoppingCallback(early_stopping_patience = 30)]
)

trainer2.train()

# os.chdir('/content/drive/MyDrive/XRA')
os.getcwd()

torch.save(trainer2.model.state_dict(), 'vit_base.pt')

from monai.networks.utils import one_hot

dice_mm = mm.DiceMetric(include_background=False, reduction = 'mean', get_not_nans=False)

def get_metrics(y_pred, y_true):

    # for sklearn
    y_pred_np = y_pred.numpy().ravel().astype(int)
    y_true_np = y_true.numpy().ravel().astype(int)

    y_pred_tensor = y_pred
    y_true_tensor = y_true

    # compute metrics
    acc = accuracy_score(y_pred_np, y_true_np)
    f1 = f1_score(y_pred_np, y_true_np, zero_division = 0)
    prec = precision_score(y_pred_np, y_true_np, zero_division = 0)
    rec = recall_score(y_pred_np, y_true_np, zero_division = 0)
    js = jaccard_score(y_pred_np, y_true_np, zero_division = 0)

    # dice and hausdorff scores
    # pred_fg = y_pred
    # pred_bg = 1 - y_pred
    # pred_dice = torch.cat([pred_bg, pred_fg], dim = 1)

    # dice_score = DiceScore(num_classes = 2, include_background = False)
    # dice = dice_score(y_pred_tensor, y_true_tensor)

    y_pred_1hot = one_hot(y_pred, num_classes = 2)
    y_true_1hot = one_hot(y_true, num_classes = 2)

    dice_score = dice_mm(y_pred_1hot, y_true_1hot)
    dice = dice_mm.aggregate().item()





    # hausdorff = HausdorffDistance(num_classes = 2, include_background = False,
    #                               distance_metric = 'euclidean', directed = False)
    # hd = hausdorff(y_pred_tensor, y_true_tensor)

    res = {'acc': acc, 'dice': dice, 'f1': f1, 'rec': rec, 'prec': prec, 'jacc': js }

    return res

# visnr.to(device = 'cpu')
vitp2.to(device = 'cpu')
y_pred = vitp2.predict(test_x, threshold = 0.6)
y = test_y

get_metrics(y_pred, y)

y_pred.shape

color_sequences = ['viridis', 'plasma', 'inferno', 'magma', 'cividis']
for i in range(10):
    pred_mask = y_pred[i].squeeze().numpy()
    plt.imshow(pred_mask, cmap = 'viridis',
               vmax = 1.0, vmin = 0.0, interpolation = 'nearest')
    plt.show()

# os.chdir('/content/drive/MyDrive/XRA')
out_dir = 'vit_pred_masks'
os.makedirs('vit_pred_masks')

for i in range(len(y_pred)):

    pred_mask = y_pred[i].squeeze().numpy()

    plt.imshow(pred_mask, cmap = 'gray',
               vmax = 1.0, vmin = 0.0, interpolation = 'nearest')

    plt.axis('off')
    plt.subplots_adjust(left = 0, right = 1, bottom = 0, top = 1)

    plt.savefig(os.path.join(out_dir, f"pred_{i}.png"))
    plt.close()